{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912\n",
      "(768, 9)\n",
      "[[   6.     148.      72.    ...,   33.6      0.627   50.   ]\n",
      " [   1.      85.      66.    ...,   26.6      0.351   31.   ]\n",
      " [   8.     183.      64.    ...,   23.3      0.672   32.   ]\n",
      " ..., \n",
      " [   5.     121.      72.    ...,   26.2      0.245   30.   ]\n",
      " [   1.     126.      60.    ...,   30.1      0.349   47.   ]\n",
      " [   1.      93.      70.    ...,   30.4      0.315   23.   ]]\n",
      "(768, 8)\n",
      "---\n",
      "[ 1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.\n",
      "  0.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.\n",
      "  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  0.  1.\n",
      "  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  1.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "  0.  1.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  1.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  1.\n",
      "  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.\n",
      "  1.  0.  1.  1.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  1.  1.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.\n",
      "  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  0.  1.  0.\n",
      "  0.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.\n",
      "  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.\n",
      "  1.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  1.  0.  1.  0.\n",
      "  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  1.\n",
      "  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  1.  1.  0.\n",
      "  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  1.  0.  1.\n",
      "  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.\n",
      "  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  1.  1.  1.\n",
      "  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.]\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "#加载训练数据\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 读取csv文件里的数据\n",
    "\n",
    "dataset = np.loadtxt(\"./data/pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "print dataset.size\n",
    "print dataset.shape\n",
    "\n",
    "# 把dataset的数据按照输入及输出分成两个数据集（dataset前8位为输入，第9位输出）\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "\n",
    "Y = dataset[:,8]\n",
    "\n",
    "print X\n",
    "#查看各维度的大小\n",
    "print X.shape \n",
    "print '---'\n",
    "print Y\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_31 (Dense)                   (None, 12)          108         dense_input_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                   (None, 8)           104         dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                   (None, 1)           9           dense_32[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 221\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "Dense层：全连接层\n",
    "\n",
    "keras.layers.core.Dense(output_dim,input_dim=None,activation='linear')\n",
    "\n",
    "output_dim：\n",
    "大于0的整数，代表该层的输出维度。\n",
    "\n",
    "input_dim：\n",
    "整数，输入数据的维度。当Dense层作为网络的第一层时，必须指定该参数或input_shape参数。\n",
    "模型中非首层的全连接层其输入维度可以自动推断，因此非首层的全连接定义时不需要指定输入维度。\n",
    "\n",
    "activation：\n",
    "激活函数，为预定义的激活函数名，或逐元素（element-wise）的Theano函数。\n",
    "如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）。\n",
    "\n",
    "'''\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\xe5\\x9c\\xa8 Sebastian Ruder \\xe7\\x9a\\x84\\xe8\\xbf\\x99\\xe7\\xaf\\x87\\xe8\\xae\\xba\\xe6\\x96\\x87\\xe4\\xb8\\xad\\xe7\\xbb\\x99\\xe5\\x87\\xba\\xe4\\xba\\x86\\xe5\\xb8\\xb8\\xe7\\x94\\xa8\\xe4\\xbc\\x98\\xe5\\x8c\\x96\\xe5\\x99\\xa8\\xe7\\x9a\\x84\\xe6\\xaf\\x94\\xe8\\xbe\\x83\\xef\\xbc\\x8c\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9\\xe6\\x9d\\xa5\\xe5\\xad\\xa6\\xe4\\xb9\\xa0\\xe4\\xb8\\x80\\xe4\\xb8\\x8b\\xef\\xbc\\x9a\\nhttps://arxiv.org/pdf/1609.04747.pdf\\n\\nhttps://www.jianshu.com/p/d99b83f4c1a6\\n\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#优化器optimizer\n",
    "#目标函数，或称损失函数loss\n",
    "#指标列表metrics,对分类问题，我们一般将该列表设置为metrics=['accuracy']。\n",
    "'''\n",
    "在 Sebastian Ruder 的这篇论文中给出了常用优化器的比较，今天来学习一下：\n",
    "https://arxiv.org/pdf/1609.04747.pdf\n",
    "\n",
    "https://www.jianshu.com/p/d99b83f4c1a6\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd39de66fd0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果使用的是Jupyter 记得把verbose=0加上\n",
    "# loss 是训练集损失值.  acc 是训练集准确率。\n",
    "model.fit(X,Y,nb_epoch=120, batch_size=32,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n",
      "Help on method fit in module keras.models:\n",
      "\n",
      "fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, **kwargs) method of keras.models.Sequential instance\n",
      "    Trains the model for a fixed number of epochs.\n",
      "    \n",
      "    # Arguments\n",
      "        x: input data, as a Numpy array or list of Numpy arrays\n",
      "            (if the model has multiple inputs).\n",
      "        y: labels, as a Numpy array.\n",
      "        batch_size: integer. Number of samples per gradient update.\n",
      "        nb_epoch: integer, the number of epochs to train the model.\n",
      "        verbose: 0 for no logging to stdout,\n",
      "            1 for progress bar logging, 2 for one log line per epoch.\n",
      "        callbacks: list of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: float (0. < x < 1).\n",
      "            Fraction of the data to use as held-out validation data.\n",
      "        validation_data: tuple (X, y) to be used as held-out\n",
      "            validation data. Will override validation_split.\n",
      "        shuffle: boolean or str (for 'batch').\n",
      "            Whether to shuffle the samples at each epoch.\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "        class_weight: dictionary mapping classes to a weight value,\n",
      "            used for scaling the loss function (during training only).\n",
      "        sample_weight: Numpy array of weights for\n",
      "            the training samples, used for scaling the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape (samples, sequence_length),\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            sample_weight_mode=\"temporal\" in compile().\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "翻译的教程是较新的keras，但是我用的docker镜像的keras没有升级，所以出错了，这边可以通过python的help方法查看keras的model.fit的说明\n",
    "原来keras 1.0.3中fit的参数里epochs是写成nb_epoch的。\n",
    "'''\n",
    "\n",
    "import keras\n",
    "print keras.__version__\n",
    "print help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s     \n",
      "('acc', 70.052083333333343)\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, Y)\n",
    "print(model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "z=X[0:10,]\n",
    "\n",
    "predictions = model.predict(z)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=np.array([[10,115,0,0,0,35.3,0.134,29]])\n",
    "round(model.predict(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
